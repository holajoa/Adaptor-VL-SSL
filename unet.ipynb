{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from segmentation_models_pytorch.encoders._base import EncoderMixin\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchxrayvision as xrv\n",
    "\n",
    "from models.adaptor import Adaptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _ResNetAE(nn.Module):\n",
    "    def __init__(self, adaptor, downblock, upblock, num_layers, n_classes):\n",
    "        super(_ResNetAE, self).__init__()\n",
    "\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_downlayer(downblock, 64, num_layers[0])\n",
    "        self.layer2 = self._make_downlayer(downblock, 128, num_layers[1], stride=2)\n",
    "        self.layer3 = self._make_downlayer(downblock, 256, num_layers[2], stride=2)\n",
    "        self.layer4 = self._make_downlayer(downblock, 128, num_layers[3], stride=6)\n",
    "\n",
    "        self.adaptor = adaptor\n",
    "        \n",
    "        self.uplayer0 = nn.ConvTranspose2d(768, 512, kernel_size=3, stride=1, bias=False)\n",
    "        \n",
    "        self.uplayer1 = self._make_up_block(upblock, 128, num_layers[3], stride=6)\n",
    "        self.uplayer2 = self._make_up_block(upblock, 64, num_layers[2], stride=2)\n",
    "        self.uplayer3 = self._make_up_block(upblock, 32, num_layers[1], stride=2)\n",
    "        self.uplayer4 = self._make_up_block(upblock, 16, num_layers[0], stride=2)\n",
    "\n",
    "        upsample = nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.in_channels, 64, kernel_size=1, stride=2, bias=False, output_padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "        self.uplayer_top = DeconvBottleneck(self.in_channels, 64, 1, 2, upsample)\n",
    "\n",
    "        self.conv1_1 = nn.ConvTranspose2d(64, n_classes, kernel_size=1, stride=1, bias=False)\n",
    "        \n",
    "    def _make_downlayer(self, block, init_channels, num_layer, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != init_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, init_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(init_channels * block.expansion),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, init_channels, stride, downsample))\n",
    "        self.in_channels = init_channels * block.expansion\n",
    "        for i in range(1, num_layer):\n",
    "            layers.append(block(self.in_channels, init_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_up_block(self, block, init_channels, num_layer, stride=1):\n",
    "        # upsample = None\n",
    "        # # expansion = block.expansion\n",
    "        # if stride != 1 or self.in_channels != init_channels * 2:\n",
    "        #     upsample = nn.Sequential(\n",
    "        #         nn.ConvTranspose2d(self.in_channels, init_channels * 2, kernel_size=1, stride=stride, bias=False, output_padding=1),\n",
    "        #         nn.BatchNorm2d(init_channels * 2),\n",
    "        #     )\n",
    "        # layers = []\n",
    "        # for i in range(1, num_layer):\n",
    "        #     layers.append(block(self.in_channels, init_channels, 4))\n",
    "\n",
    "        # layers.append(block(self.in_channels, init_channels, 2, stride, upsample))\n",
    "        # self.in_channels = init_channels * 2\n",
    "        # return nn.Sequential(*layers)\n",
    "        up_block = _UpBlock(self.in_channels, block, init_channels, num_layer, stride)\n",
    "        self.in_channels = up_block.in_channels\n",
    "        return up_block\n",
    "    \n",
    "    def encode(self, x, check_resolution=True):\n",
    "        if check_resolution and hasattr(self, 'weights_metadata'):\n",
    "            resolution = self.weights_metadata['resolution']\n",
    "            if (x.shape[2] != resolution) | (x.shape[3] != resolution):\n",
    "                raise ValueError(\"Input size ({}x{}) is not the native resolution ({}x{}) for this model. Set check_resolution=False on the encode function to override this error.\".format(x.shape[2], x.shape[3], resolution, resolution))\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = feat1 = self.layer1(x)\n",
    "        x = feat2 = self.layer2(x)\n",
    "        x = feat3 = self.layer3(x)\n",
    "        x = feat4 = self.layer4(x)\n",
    "        return x, [feat1, feat2, feat3, feat4]\n",
    "\n",
    "    def fusion(self, x):\n",
    "        x = self.adaptor(x)\n",
    "        return x\n",
    "    \n",
    "    def upsample(self, x):\n",
    "        x = self.uplayer0(x)\n",
    "        return x\n",
    "    \n",
    "    def decode(self, x, skips, image_size=[1, 1, 224, 224]):\n",
    "        x = self.uplayer1(x, skips[3])\n",
    "        x = self.uplayer2(x, skips[2])\n",
    "        x = self.uplayer3(x, skips[1])\n",
    "        x = self.uplayer4(x, skips[0])\n",
    "        x = self.uplayer_top(x, skips)\n",
    "\n",
    "        x = self.conv1_1(x, output_size=image_size)\n",
    "        return x\n",
    "\n",
    "    def get_global_features(self, x):\n",
    "        return torch.flatten(x, start_dim=2).permute((0, 2, 1)).mean(1) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        ret = {}\n",
    "        ret[\"pre_adaptor\"], ret['skips'] = self.encode(x)\n",
    "        ret['global_features'] = z = self.get_global_features(ret[\"pre_adaptor\"])\n",
    "        ret[\"multimodal_features\"] = z = self.fusion(z)\n",
    "        ret[\"out\"] = self.decode(self.upsample(z.unsqueeze(2).unsqueeze(3)), ret['skips'], x.size())\n",
    "\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _ResNetAEEncoder(nn.Module):\n",
    "    def __init__(self, adaptor, downblock, upblock, num_layers, n_classes):\n",
    "        super(_ResNetAE, self).__init__()\n",
    "\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_downlayer(downblock, 64, num_layers[0])\n",
    "        self.layer2 = self._make_downlayer(downblock, 128, num_layers[1], stride=2)\n",
    "        self.layer3 = self._make_downlayer(downblock, 256, num_layers[2], stride=2)\n",
    "        self.layer4 = self._make_downlayer(downblock, 128, num_layers[3], stride=6)\n",
    "\n",
    "        self.adaptor = adaptor\n",
    "        \n",
    "        self.uplayer0 = nn.ConvTranspose2d(768, 512, kernel_size=3, stride=1, bias=False)\n",
    "        \n",
    "        self.uplayer1 = self._make_up_block(upblock, 128, num_layers[3], stride=6)\n",
    "        self.uplayer2 = self._make_up_block(upblock, 64, num_layers[2], stride=2)\n",
    "        self.uplayer3 = self._make_up_block(upblock, 32, num_layers[1], stride=2)\n",
    "        self.uplayer4 = self._make_up_block(upblock, 16, num_layers[0], stride=2)\n",
    "\n",
    "        upsample = nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.in_channels, 64, kernel_size=1, stride=2, bias=False, output_padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "        self.uplayer_top = DeconvBottleneck(self.in_channels, 64, 1, 2, upsample)\n",
    "\n",
    "        self.conv1_1 = nn.ConvTranspose2d(64, n_classes, kernel_size=1, stride=1, bias=False)\n",
    "        \n",
    "    def _make_downlayer(self, block, init_channels, num_layer, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != init_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, init_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(init_channels * block.expansion),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, init_channels, stride, downsample))\n",
    "        self.in_channels = init_channels * block.expansion\n",
    "        for i in range(1, num_layer):\n",
    "            layers.append(block(self.in_channels, init_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_up_block(self, block, init_channels, num_layer, stride=1):\n",
    "        # upsample = None\n",
    "        # # expansion = block.expansion\n",
    "        # if stride != 1 or self.in_channels != init_channels * 2:\n",
    "        #     upsample = nn.Sequential(\n",
    "        #         nn.ConvTranspose2d(self.in_channels, init_channels * 2, kernel_size=1, stride=stride, bias=False, output_padding=1),\n",
    "        #         nn.BatchNorm2d(init_channels * 2),\n",
    "        #     )\n",
    "        # layers = []\n",
    "        # for i in range(1, num_layer):\n",
    "        #     layers.append(block(self.in_channels, init_channels, 4))\n",
    "\n",
    "        # layers.append(block(self.in_channels, init_channels, 2, stride, upsample))\n",
    "        # self.in_channels = init_channels * 2\n",
    "        # return nn.Sequential(*layers)\n",
    "        up_block = _UpBlock(self.in_channels, block, init_channels, num_layer, stride)\n",
    "        self.in_channels = up_block.in_channels\n",
    "        return up_block\n",
    "    \n",
    "    def encode(self, x, check_resolution=True):\n",
    "        if check_resolution and hasattr(self, 'weights_metadata'):\n",
    "            resolution = self.weights_metadata['resolution']\n",
    "            if (x.shape[2] != resolution) | (x.shape[3] != resolution):\n",
    "                raise ValueError(\"Input size ({}x{}) is not the native resolution ({}x{}) for this model. Set check_resolution=False on the encode function to override this error.\".format(x.shape[2], x.shape[3], resolution, resolution))\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = feat1 = self.layer1(x)\n",
    "        x = feat2 = self.layer2(x)\n",
    "        x = feat3 = self.layer3(x)\n",
    "        x = feat4 = self.layer4(x)\n",
    "        return x, [feat1, feat2, feat3, feat4]\n",
    "\n",
    "    def fusion(self, x):\n",
    "        x = self.adaptor(x)\n",
    "        return x\n",
    "    \n",
    "    def upsample(self, x):\n",
    "        x = self.uplayer0(x)\n",
    "        return x\n",
    "    \n",
    "    def decode(self, x, skips, image_size=[1, 1, 224, 224]):\n",
    "        x = self.uplayer1(x, skips[3])\n",
    "        x = self.uplayer2(x, skips[2])\n",
    "        x = self.uplayer3(x, skips[1])\n",
    "        x = self.uplayer4(x, skips[0])\n",
    "        x = self.uplayer_top(x, skips)\n",
    "\n",
    "        x = self.conv1_1(x, output_size=image_size)\n",
    "        return x\n",
    "\n",
    "    def get_global_features(self, x):\n",
    "        return torch.flatten(x, start_dim=2).permute((0, 2, 1)).mean(1) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        ret = {}\n",
    "        ret[\"pre_adaptor\"], ret['skips'] = self.encode(x)\n",
    "        ret['global_features'] = z = self.get_global_features(ret[\"pre_adaptor\"])\n",
    "        ret[\"multimodal_features\"] = z = self.fusion(z)\n",
    "        ret[\"out\"] = self.decode(self.upsample(z.unsqueeze(2).unsqueeze(3)), ret['skips'], x.size())\n",
    "\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (512) must match the size of tensor b (1024) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[39m=\u001b[39m _ResNetAE(adaptor, Bottleneck, DeconvBottleneck, [\u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m23\u001b[39m, \u001b[39m2\u001b[39m], \u001b[39m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m out \u001b[39m=\u001b[39m model(x)\n",
      "File \u001b[0;32m/vol/bitbucket/jq619/idv/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[111], line 111\u001b[0m, in \u001b[0;36m_ResNetAE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m ret[\u001b[39m'\u001b[39m\u001b[39mglobal_features\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_global_features(ret[\u001b[39m\"\u001b[39m\u001b[39mpre_adaptor\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    110\u001b[0m ret[\u001b[39m\"\u001b[39m\u001b[39mmultimodal_features\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfusion(z)\n\u001b[0;32m--> 111\u001b[0m ret[\u001b[39m\"\u001b[39m\u001b[39mout\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupsample(z\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m2\u001b[39;49m)\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m3\u001b[39;49m)), ret[\u001b[39m'\u001b[39;49m\u001b[39mskips\u001b[39;49m\u001b[39m'\u001b[39;49m], x\u001b[39m.\u001b[39;49msize())\n\u001b[1;32m    113\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "Cell \u001b[0;32mIn[111], line 94\u001b[0m, in \u001b[0;36m_ResNetAE.decode\u001b[0;34m(self, x, skips, image_size)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, x, skips, image_size\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m]):\n\u001b[0;32m---> 94\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muplayer1(x, skips[\u001b[39m3\u001b[39;49m])\n\u001b[1;32m     95\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muplayer2(x, skips[\u001b[39m2\u001b[39m])\n\u001b[1;32m     96\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muplayer3(x, skips[\u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m/vol/bitbucket/jq619/idv/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[110], line 23\u001b[0m, in \u001b[0;36m_UpBlock.forward\u001b[0;34m(self, x, skip)\u001b[0m\n\u001b[1;32m     21\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([x, skip], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     22\u001b[0m     \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup_blocks:\n\u001b[0;32m---> 23\u001b[0m         x \u001b[39m=\u001b[39m block(x) \n\u001b[1;32m     24\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n\u001b[1;32m     25\u001b[0m \u001b[39melse\u001b[39;00m:\u001b[39m#            \u001b[39;00m\n",
      "File \u001b[0;32m/vol/bitbucket/jq619/idv/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[73], line 42\u001b[0m, in \u001b[0;36mDeconvBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupsample \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m     shortcut \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupsample(x)\n\u001b[0;32m---> 42\u001b[0m out \u001b[39m=\u001b[39m out \u001b[39m+\u001b[39;49m shortcut\n\u001b[1;32m     43\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n\u001b[1;32m     45\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (512) must match the size of tensor b (1024) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "ckpt = \"/vol/bitbucket/jq619/individual-project/trained_models/pretrain/resnet-ae_clinicalbert/adaptor pretrain/cuxhbf0j/checkpoints/epoch=49-step=88799.ckpt\"\n",
    "adaptor = Adaptor.load_from_checkpoint(ckpt)\n",
    "model = _ResNetAE(adaptor, Bottleneck, DeconvBottleneck, [3, 4, 23, 2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def up_conv(in_channels, out_channels):\n",
    "    return nn.ConvTranspose2d(\n",
    "        in_channels, out_channels, kernel_size=2, stride=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetAEUNet(nn.Module):\n",
    "    def __init__(self, pretrained=False, out_channels=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.resnet_ae = xrv.autoencoders.ResNetAE(weights=\"101-elastic\")\n",
    "        self.encoder_layers = list(self.resnet_ae.children())[:8]\n",
    "        self.block1 = nn.Sequential(*self.encoder_layers[:3])\n",
    "        self.block2 = nn.Sequential(*self.encoder_layers[3:5])\n",
    "        self.block3 = nn.Sequential(*self.encoder_layers[5])\n",
    "        self.block4 = nn.Sequential(*self.encoder_layers[6])\n",
    "        self.block5 = nn.Sequential(*self.encoder_layers[7])\n",
    "        \n",
    "        self.neck = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2)\n",
    "\n",
    "        self.up_conv6 = up_conv(512, 512)\n",
    "        self.conv6 = double_conv(512 + 1024, 512)\n",
    "        self.up_conv7 = up_conv(512, 256)\n",
    "        self.conv7 = double_conv(256 + 512, 256)\n",
    "        self.up_conv8 = up_conv(256, 128)\n",
    "        self.conv8 = double_conv(128 + 256, 128)\n",
    "        self.up_conv9 = up_conv(128, 64)\n",
    "        self.conv9 = double_conv(64 + 64, 64)\n",
    "        self.up_conv10 = up_conv(64, 32)\n",
    "        self.conv10 = nn.Conv2d(32, out_channels, kernel_size=1)\n",
    "        \n",
    "        if not pretrained:\n",
    "            self._weights_init()\n",
    "        \n",
    "    def _weights_init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        block1 = self.block1(x)\n",
    "        block2 = self.block2(block1)\n",
    "        block3 = self.block3(block2)\n",
    "        block4 = self.block4(block3)\n",
    "        block5 = self.block5(block4)\n",
    "\n",
    "        neck = self.neck(block5)\n",
    "        \n",
    "        x = self.up_conv6(neck)\n",
    "        \n",
    "        x = torch.cat([x, block4], dim=1)\n",
    "        x = self.conv6(x)\n",
    "\n",
    "        x = self.up_conv7(x)\n",
    "        x = torch.cat([x, block3], dim=1)\n",
    "        x = self.conv7(x)\n",
    "\n",
    "        x = self.up_conv8(x)\n",
    "        x = torch.cat([x, block2], dim=1)\n",
    "        x = self.conv8(x)\n",
    "\n",
    "        x = self.up_conv9(x)\n",
    "        x = torch.cat([x, block1], dim=1)\n",
    "        x = self.conv9(x)\n",
    "\n",
    "        x = self.up_conv10(x)\n",
    "        x = self.conv10(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([1, 64, 112, 112]), torch.Size([1, 256, 56, 56]), torch.Size([1, 512, 28, 28]), torch.Size([1, 1024, 14, 14]), torch.Size([1, 512, 3, 3]), torch.Size([1, 512, 7, 7])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 224, 224])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNetAEUNet()\n",
    "x = torch.ones(1, 1, 224, 224)\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
