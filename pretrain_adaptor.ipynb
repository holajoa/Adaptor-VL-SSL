{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataset.dataset import (\n",
    "    MultimodalPretrainedEmbeddingsDatasetLoader, \n",
    "    MultimodalPretrainedEmbeddingsDataset, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_embeddings_dir = '/vol/bitbucket/jq619/individual-project/saved_embeddings'\n",
    "\n",
    "text_embeds_raw_dir = os.path.join(saved_embeddings_dir, 'text_embeds', 'ClinicalBERT')\n",
    "image_embeds_raw_dir = os.path.join(saved_embeddings_dir, 'image_embeds', 'Swin-Base')\n",
    "\n",
    "dataset_loader = MultimodalPretrainedEmbeddingsDatasetLoader(text_embeds_raw_dir, image_embeds_raw_dir, \n",
    "                                                             split='train', num_of_batches=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 12.56it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset_loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_dict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from models.adaptor import Adaptor\n",
    "from models.configurations import (\n",
    "    TEXT_PRETRAINED_AVAILABLE,\n",
    "    VISION_PRETRAINED_AVAILABLE,\n",
    "    VISION_MODEL_TYPE_2_DATA_TRANSFORM,\n",
    "    VISION_MODEL_TYPE_2_VISION_OUTPUT_DIM, \n",
    ")\n",
    "from utils.utils import load_timm_model, freeze_encoder\n",
    "from utils.model_utils import load_vision_model\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertModel, AutoModel, ViTImageProcessor\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = Namespace(\n",
    "    **{'batch_size': 16,\n",
    "    'vision_pretrained': 'swin_base_patch4_window7_224', \n",
    "    'vision_model_type': 'timm', \n",
    "    'text_pretrained': './weights/ClinicalBERT_checkpoint/ClinicalBERT_pretraining_pytorch_checkpoint',\n",
    "    'num_train_epochs':1, \n",
    "    'lr': 1e-4,\n",
    "    'projection_dim': 768,\n",
    "    'num_hidden_layers': 1,\n",
    "    'seed':1117, }  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def get_train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Returns the training [`~torch.utils.data.DataLoader`].\n",
    "\n",
    "        Will use no sampler if `train_dataset` does not implement `__len__`, a random sampler (adapted to distributed\n",
    "        training if necessary) otherwise.\n",
    "\n",
    "        Subclass and override this method if you want to inject some custom behavior.\n",
    "        \"\"\"\n",
    "        from transformers.trainer_utils import seed_worker\n",
    "        \n",
    "        if self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "\n",
    "        train_dataset = self.train_dataset\n",
    "        data_collator = self.data_collator\n",
    "\n",
    "        train_sampler = self._get_train_sampler()\n",
    "\n",
    "        return DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self._train_batch_size,\n",
    "            sampler=train_sampler,\n",
    "            collate_fn=data_collator,\n",
    "            drop_last=self.args.dataloader_drop_last,\n",
    "            num_workers=self.args.dataloader_num_workers,\n",
    "            pin_memory=self.args.dataloader_pin_memory,\n",
    "            worker_init_fn=seed_worker,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./weights/ClinicalBERT_checkpoint/ClinicalBERT_pretraining_pytorch_checkpoint were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 09:48, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.780400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.767800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.380100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.651000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.454500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.262600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.193700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.143600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.090300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.101000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.069500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.053800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.068000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.977800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.983100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.976300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.885000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.919800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.864600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.864600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=400, training_loss=1.3243766689300538, metrics={'train_runtime': 589.8209, 'train_samples_per_second': 10.851, 'train_steps_per_second': 0.678, 'total_flos': 0.0, 'train_loss': 1.3243766689300538, 'epoch': 1.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "### Load vision model\n",
    "if args.vision_pretrained in VISION_PRETRAINED_AVAILABLE.keys():\n",
    "    assert VISION_PRETRAINED_AVAILABLE[args.vision_pretrained] == args.vision_model_type, \\\n",
    "        'Vision model type does not match pretrained model'\n",
    "vision_model = load_vision_model(args.vision_model_type, args.vision_pretrained)\n",
    "\n",
    "### Load text model\n",
    "text_model = BertModel.from_pretrained(args.text_pretrained)\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.text_pretrained)\n",
    "\n",
    "### Define model\n",
    "add_cls_token = args.vision_model_type == 'ae'\n",
    "vision_output_dim = VISION_MODEL_TYPE_2_VISION_OUTPUT_DIM[args.vision_model_type]\n",
    "model = Adaptor(\n",
    "    text_model=text_model,\n",
    "    vision_model=vision_model,\n",
    "    vision_model_type=args.vision_model_type, \n",
    "    vision_output_dim=vision_output_dim,\n",
    "    projection_dim=args.projection_dim,\n",
    "    num_hidden_layers=args.num_hidden_layers, \n",
    "    add_cls_token=add_cls_token,\n",
    ")\n",
    "freeze_encoder(model)  # freeze encoder\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "### Training\n",
    "arguments = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=args.batch_size, \n",
    "    per_device_eval_batch_size=args.batch_size,  \n",
    "    num_train_epochs=args.num_train_epochs,\n",
    "    logging_steps=20, \n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=args.lr, \n",
    "    seed=args.seed, \n",
    "    push_to_hub=False, \n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model, \n",
    "    args=arguments,\n",
    "    train_dataset=train_dataset, \n",
    "    # eval_dataset=val_dataset, \n",
    "    # tokenizer=tokenizer, \n",
    "    data_collator=None, \n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
