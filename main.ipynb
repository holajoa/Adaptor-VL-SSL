{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from typing import List, Union, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from models import FusionModule\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import (\n",
    "    VisionTextDualEncoderModel,\n",
    "    VisionTextDualEncoderProcessor,\n",
    "    AutoImageProcessor,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from transformers import VisionTextDualEncoderConfig, AutoConfig\n",
    "from transformers.models.clip.modeling_clip import CLIPOutput\n",
    "\n",
    "from models import FusionModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_pretrained = \"google/vit-base-patch16-224\"\n",
    "text_pretraind = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(text_pretraind)\n",
    "image_processor = AutoImageProcessor.from_pretrained(vision_pretrained)\n",
    "processor = VisionTextDualEncoderProcessor(image_processor, tokenizer)\n",
    "\n",
    "text_config = AutoConfig.from_pretrained(text_pretraind).to_dict()\n",
    "vision_config = AutoConfig.from_pretrained(vision_pretrained).to_dict()\n",
    "\n",
    "config = VisionTextDualEncoderConfig(\n",
    "    text_config=text_config, \n",
    "    vision_config=vision_config,\n",
    "    projection_dim=512,\n",
    ")\n",
    "\n",
    "dual_encoder = VisionTextDualEncoderModel(config)\n",
    "fusioner = FusionModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrastive training\n",
    "urls = [\n",
    "    \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n",
    "    \"https://farm3.staticflickr.com/2674/5850229113_4fe05d5265_z.jpg\",\n",
    "]\n",
    "images = [Image.open(requests.get(url, stream=True).raw) for url in urls]\n",
    "inputs = processor(\n",
    "    text=[\"a photo of a cat\", \"a photo of a dog\"], \n",
    "    images=images, return_tensors=\"pt\", padding=True\n",
    ")\n",
    "\n",
    "# inference\n",
    "encoder_outputs: CLIPOutput = dual_encoder(**inputs)\n",
    "text_embeds = encoder_outputs.text_model_output.last_hidden_state\n",
    "image_embeds = encoder_outputs.vision_model_output.last_hidden_state\n",
    "\n",
    "outputs = fusioner(text_embeds, image_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
